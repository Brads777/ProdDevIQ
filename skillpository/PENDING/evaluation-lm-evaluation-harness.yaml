# Discovered by skillpository-manager scan
name: "evaluation-lm-evaluation-harness"
type: skill
source_repo: "davila7/claude-code-templates"
source_path: "cli-tool/components/skills/ai-research/evaluation-lm-evaluation-harness/SKILL.md"
source_url: "https://github.com/davila7/claude-code-templates/tree/main/cli-tool/components/skills/ai-research/evaluation-lm-evaluation-harness"
description: "Evaluates LLMs across 60+ academic benchmarks (MMLU, HumanEval, GSM8K, TruthfulQA, HellaSwag). Use when benchmarking model quality, comparing models, reporting academic results, or tracking training progress. Industry standard used by EleutherAI, HuggingFace, and major labs. Supports HuggingFace, vLLM, APIs."
discovered: "2026-02-07 22:29"
label: "davila7 Templates"
status: pending
